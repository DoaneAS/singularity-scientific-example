# Primer: What are containers, and how can they be used to increase research integrity in the sciences?

## Cameron J. Prybol<sup>1</sup>, Euan A. Ashley<sup>1, 2, 3</sup>

### <sup>1</sup> Department of Genetics, Stanford University, Stanford, CA, 94305, USA., <sup>2</sup>Department of Medicine, Stanford University, Stanford, CA, 94305, USA., <sup>3</sup>Stanford Center for Inherited Cardiovascular Disease, Stanford University, Stanford, CA, 94305, USA.

## Abstract

As computational biologists continue to push the performance boundaries of high-performance computing (HPC) clusters with ever-growing datasets and algorithmic complexity, the process of configuring and maintaining these computing environments also grows in complexity. Without a sufficiently similar computing environment (i.e. same software, version numbers, operating system, shell environment) it can be difficult, if not impossible, to replicate results, even when given the same input code and datasets. In addition to enabling others to reproduce your work, reproducible computing environments also enable researchers to more reliably archive their own projects. As system administrators make upgrades to computing infrastructure to improve reliability, patch security issues, and enhance user experience, researchers may not be able to replicate their own results only months after the completion of a project. Containers are isolated, fully-functional computing environments that abstract entire operating systems, along with all configurations and software, into single files. Not only do these containers provide researchers with more freedom and flexibility to configure a computing environment that best suits their needs, but containers also allow researchers to share and archive computing enviroments as easily as they would share and archive any other file. All in all, containers enable researchers to spend more time focusing on research and less time troubleshooting software issues, while at the same time promoting transparancy and integrity in research by lowering the barrier for others to replicate and review published results.

## Introduction

Reproducibility is at the core of scientific philosophy, yet many well-respected journals still accept and publish computational analyses with methods sections that are far too incomplete to replicate results. Even in cases where the software used, commands executed, and versions required are meticulously curated for the final publication, configuring a computing enviroment that precisely matches the one used in the publication can be a research project in and of itself. Given that computers, both by definition and by design, and meant to recieve instructions and execute those instructions to yield a deterministic result, it may seem counter-intuitive that many wet-lab bench analyses are easier to replicate than computational ones. One need look no further than the constrast in how young scientists percieve learning to perform a polymerase-chain reaction (PCR) amplification, a technique developed in the mid-1980's, to how young scientists percieve the task of learning to use Unix-based operating systems, which have existed since the mid- to late-1960's. Both are irreplacable tools in research, yet learning how to properly utilize the latter is often a far more daunting task, requiring months to years longer to master than the former.

A major factor in the percieved ease-of-use of learning to perform a successful PCR amplification is the availability of user-friendly web services like Primer3Plus [cite] and NCBI's Primer-BLAST [cite], among others. It is then usually a user-friendly process to program and run the reaction using a thermal cycling machine. Whether through lab- or university-sponsors cloud computing accounts, or on-site HPC clusters at large research institutions, researchers are often able to acquire access to the hardware they need to perform computational analyses, yet there is no equivalently user-friendly process to configuring and install software into an environment to prepare to run them.

Educational efforts like Software Carpentry and the Mozilla Science Lab have evangelized best-practices used by software developers in order to improve computational literacy and reproducibility across the sciences. Open-source platforms like the web-facing Galaxy project, command-line interface (CLI)-facing bcbio-nextgen, and commercial services like DNAnexus enable users to run analyses using customizable scripts and rigorously validated pipelines and software. However, these services are focused on specific use-cases within the domain of computional biology, which can make them cumbersome for novel analyses and limits their adoption across disciplines. As many universities and research institutions have invested in HPC clusters, and researchers across all domains have experience writing analyses using command line applications and tools that run on linux- and unix-based operating systems, an ideal solution for reproducable research would be one that allowed researchers to use existing infrastructure and skill-sets, and does not require any modificications to their existing code.

Many researchers are already familiar with version-control software like git, mercurial, and svn that allow users to effeciently backup, collaborate on, and distribute the code they use in their projects and analyses. Many journals and funding agencies require data to be made available on archiving services like the Gene Expression Omnibus and European Nucleotide Archive. By combining these existing services with containers, or mobile, host-isolated, operating systems in a file, end-users who wish to reproduce results need only an internet connection, sufficient resources to store and process the data, and to have the container software installed. This dramatically lowers the energy and skill required for others to review and learn from your work, while also encouraging transparancy and quality of research and increasing the rate of knowledge transfer. Additionally, it reduces the burden on the research staff who maintain these HPC clusters by allowing them to focus on maintaining only the software required to run the containers and maximizing the performance of the cluster, rather than also having to maintain the diverse software requirements across research groups.

Here we introduce Singularity, a container platform specifically designed for the shared computing environments of HPC clusters at large research institutions. See the Discussion section for details why we chose to perform this analysis on Singularity, rather than other container platforms.

# Methods

To compare the overhead of running analyses using isolated environments within Singularity containers to running analyses using software installed directly onto the host, we performed several iterations of two benchmarks. The first benchmark quantifies transcript abundances of >68 million 2x75bp reads (Human polyA+ total RNA, GM12878 cell line) from round 1 of the RNA-seq Genome Annotation Assessment Project (http://www.gencodegenes.org/rgasp/data.html) using kallisto [cite]. The second benchmark maps 100 million 2x75bp reads to GRCh38 (ensembl release 85) using bwa [cite]. Each iteration of simulation ran the host and container tests in parallel in an attempt to capture the effects of system load as similarly as we could. Iterations were performed in serial over several days. To test a java-based application, which may have different running behavior inside of containers compared to compiled C code, we also tested the non-commercial RTG-core suite of tools by mapping the Ashkenazi trio provided by GIAB to GRCh38, and then calling variants using a pedigree-aware variant calling and haplotype-phasing.

For full details including operating systems, cpu architecture, and version numbers of all software used, see the supplementary material. The instructions for how to acquire the code and Singularity container are available at https://github.com/cjprybol/singularity-manuscript. The data download has been automated and is included in the source code.

# Results

The difference between running the job on a single node vs having it distributed across multiple nodes was far more costly than the difference between running on a single node using host vs. container software.

# Disussion

Users can install software into containers and configure them in the same way they would any other unix- or linux-based operating system. Containers can execute existing source code, often with no modifications required, which allows researchers to utilize it's added benefits of enviroment isolation and reproducability with minimal disturbance to their normal workflows. Singularity is not the only cotainer platform available to researchers. However, Singularity has many advantages over other container platforms, specifically those that have gained strong support from commercial and enterprise "cloud" computing services.

Docker is a very popular container platform that has strong enterprise backing, a well developed community, and cross-platform support. There have been several projects that have invested in Docker and it's utility to the research community. Bioboxes (cite Bioboxes: standardised containers for interchangeable bioinformatics software), BioShaDock (cite BioShaDock: a community driven bioinformatics shared Docker-based tools registry), and Biodocker (cite website) are registries for Docker containers where users can share and download ready-to-use containers. These services primarily host containers that package individual software libraries seperately, rather than including all of the software necessarily to reproduce a publication in a single container.

Unfortunately, because Docker's target audience has been commercial and enterprise usage, it was designed with the expectation that it would be run in an isolated, cloud-based computing instance. Docker runs all processes in a priviledged state. This means that processes have greater permissions levels than the user who is executing the command, i.e. regular users can submit commands that will run as root. This is a major security risk for system administrators who manage resources for multiple users on a shared cluster. Singularity was designed to address these security issues and enables all jobs to be executed with the same permissions as the user, ensuring that once a container has been configured, users can utilize the environment without posing any risk to the integrity of the directories and files owned by different users and groups. This read-only interaction allows multiple processes to use the container in parallel, whether that be multple users or simply multiple processes in a parallel job.

Additional benefits to utilizing containers includes the ability to distribute required software across multiple containers. For example, legacy software that requires outdated operating systems and dependencies can be installed into their own container(s), and modern software that have conflicting dependencies can be installed to seperate containers to eliminate library conflicts. All of these containers can be utilized on the same clusters, improving researcher and system administrator productivity by reducing the effort required to maintain environments.

**Availability**: Singularity is available at http://singularity.lbl.gov/. Examples of how to create and utilize containers for research projects are available at https://github.com/cjprybol/reproducibility-via-singularity. The repository associated with the manuscript is available at https://github.com/cjprybol/singularity-manuscript

**Contact**: euan@stanford.edu, gmkurtzer@lbl.gov
